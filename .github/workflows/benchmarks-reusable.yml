name: Benchmarks Reusable

on:
  workflow_call:
    inputs:
      str_name:
        required: true
        type: string
      preset:
        required: true
        type: string
      pr_no:
        required: true
        # even though this is a number, this is a workaround for issues with
        # reusable workflow calls that result in "Unexpected value '0'" error.
        type: string
      bench_script_params:
        required: false
        type: string
        default: ''
      sycl_config_params:
        required: false
        type: string
        default: ''
      sycl_repo:
        required: true
        type: string
        default: 'intel/llvm'
      sycl_commit:
        required: false
        type: string
        default: ''
      compute_runtime_commit:
        required: false
        type: string
        default: ''
      runner:
        required: true
        type: string

permissions:
  contents: read

jobs:
  benchmarks:
    name: Build SYCL, UR, run Compute Benchmarks
    permissions:
      contents: write
      pull-requests: write
    strategy:
      matrix:
        adapter: [
          {str_name: "${{ inputs.str_name }}",
          sycl_config: "${{ inputs.sycl_config_params }}",
          }
        ]
        build_type: [Release]

    runs-on: "${{ inputs.runner }}"

    steps:
    - name: Add comment to PR
      uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1
      if: ${{ always() && inputs.pr_no != 0 }}
      with:
        script: |
          const pr_no = '${{ inputs.pr_no }}';
          const adapter = '${{ matrix.adapter.str_name }}';
          const url = '${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}';
          const params = '${{ inputs.bench_script_params }}';
          const body = `Compute Benchmarks ${adapter} run (with params: ${params}):\n${url}`;

          github.rest.issues.createComment({
            issue_number: pr_no,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          })

    - name: Checkout benchmark scripts
      uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
      with:
        repository: igchor/llvm
        ref: 28eb6278d8c1e437909baeadc3ba796f70223bd1
        path: sc
        sparse-checkout: |
          devops/scripts/benchmarks

    - name: Checkout results branch
      uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
      with:
        ref: benchmark-results
        path: results-repo

    - name: Install Python requirements in venv
      run: |
        python -m venv .venv
        source .venv/bin/activate
        echo "$PATH" >> $GITHUB_PATH
        pip install -r sc/devops/scripts/benchmarks/requirements.txt

    - name: Checkout SYCL
      uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1
      with:
        repository: ${{inputs.sycl_repo}}
        ref: refs/heads/sycl
        path: sycl-repo
        fetch-depth: 1
        fetch-tags: false

    - name: Fetch specific SYCL commit
      if: inputs.sycl_commit != ''
      working-directory: ./sycl-repo
      run: |
        git fetch --depth=1 origin ${{ inputs.sycl_commit }}
        git checkout ${{ inputs.sycl_commit }}

    - name: Set CUDA env vars
      if: matrix.adapter.str_name == 'cuda'
      run: |
        echo "CUDA_LIB_PATH=/usr/local/cuda/lib64/stubs" >> $GITHUB_ENV
        echo "LD_LIBRARY_PATH=/usr/local/cuda/compat/:/usr/local/cuda/lib64:$LD_LIBRARY_PATH" >> $GITHUB_ENV

    - name: Configure SYCL
      run: >
        python3 sycl-repo/buildbot/configure.py
        -t ${{matrix.build_type}}
        -o ${{github.workspace}}/sycl_build
        --cmake-gen "Ninja" ${{matrix.adapter.sycl_config}}
        --cmake-opt="-DLLVM_INSTALL_UTILS=ON"
        --cmake-opt="-DSYCL_PI_TESTS=OFF"
        --cmake-opt=-DCMAKE_C_COMPILER_LAUNCHER=ccache
        --cmake-opt=-DCMAKE_CXX_COMPILER_LAUNCHER=ccache

    - name: Build SYCL
      run: cmake --build ${{github.workspace}}/sycl_build -j

    - name: Configure UR
      run: >
        cmake -DCMAKE_BUILD_TYPE=Release
        -S${{github.workspace}}/sycl-repo/unified-runtime
        -B${{github.workspace}}/ur_build
        -DCMAKE_INSTALL_PREFIX=${{github.workspace}}/ur_install
        -DUR_BUILD_TESTS=OFF
        -DUR_BUILD_ADAPTER_L0=ON
        -DUR_BUILD_ADAPTER_L0_V2=ON
        -DUMF_DISABLE_HWLOC=ON

    - name: Build UR
      run: cmake --build ${{github.workspace}}/ur_build -j $(nproc)

    - name: Install UR
      run: cmake --install ${{github.workspace}}/ur_build

    - name: Compute core range
      run: |
        # Compute the core range for the first NUMA node; second node is for UMF jobs.
        # Skip the first 4 cores - the kernel is likely to schedule more work on these.
        CORES=$(lscpu | awk '
          /NUMA node0 CPU|On-line CPU/ {line=$0}
          END {
            split(line, a, " ")
            split(a[4], b, ",")
            sub(/^0/, "4", b[1])
            print b[1]
          }')
        echo "Selected core: $CORES"
        echo "CORES=$CORES" >> $GITHUB_ENV

        ZE_AFFINITY_MASK=0
        echo "ZE_AFFINITY_MASK=$ZE_AFFINITY_MASK" >> $GITHUB_ENV

    - name: Run benchmarks
      id: benchmarks
      run: >
        taskset -c ${{ env.CORES }} ./sc/devops/scripts/benchmarks/main.py
        ~/ur_bench_workdir
        --sycl ${{ github.workspace }}/sycl_build
        --ur ${{ github.workspace }}/ur_install
        --adapter ${{ matrix.adapter.str_name }}
        --compute-runtime ${{ inputs.compute_runtime_commit }}
        --build-igc
        --output-html remote
        --results-dir ${{ github.workspace }}/results-repo
        --output-markdown
        --preset ${{ inputs.preset }}
        ${{ inputs.bench_script_params }}

    # In case it failed to add a comment, we can still print the results.
    - name: Print benchmark results
      if: ${{ always() }}
      run: cat ${{ github.workspace }}/benchmark_results.md || true

    - name: Add comment to PR
      uses: actions/github-script@60a0d83039c74a4aee543508d2ffcb1c3799cdea # v7.0.1
      if: ${{ always() && inputs.pr_no != 0 }}
      with:
        script: |
          let markdown = ""
          try {
            const fs = require('fs');
            markdown = fs.readFileSync('${{ github.workspace }}/benchmark_results.md', 'utf8');
          } catch(err) {
          }

          const pr_no = '${{ inputs.pr_no }}';
          const adapter = '${{ matrix.adapter.str_name }}';
          const url = '${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}';
          const test_status = '${{ steps.benchmarks.outcome }}';
          const job_status = '${{ job.status }}';
          const params = '${{ inputs.bench_script_params }}';
          const body = `Compute Benchmarks ${adapter} run (${params}):\n${url}\nJob status: ${job_status}. Test status: ${test_status}.\n ${markdown}`;

          github.rest.issues.createComment({
            issue_number: pr_no,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: body
          })

    - name: Commit data.json and results directory
      working-directory: results-repo
      run: |
        git config --global user.name "GitHub Actions Bot"
        git config --global user.email "actions@github.com"

        for attempt in {1..5}; do
          echo "Attempt #$attempt to push changes"

          rm -f data.json
          cp ${{ github.workspace }}/sc/devops/scripts/benchmarks/html/data.json .

          git add data.json results/
          git commit -m "Add benchmark results and data.json"

          results_file=$(git diff HEAD~1 --name-only -- results/ | head -n 1)

          if git push origin benchmark-results; then
            echo "Push succeeded"
            break
          fi

          echo "Push failed, retrying..."

          if [ -n "$results_file" ]; then
            mv $results_file ${{ github.workspace }}/temp_$(basename $results_file)

            git reset --hard origin/benchmark-results
            git pull origin benchmark-results

            new_file="results/$(basename "$results_file")"
            mv ${{ github.workspace }}/temp_$(basename $results_file) $new_file
          fi

          echo "Regenerating data.json"
          # --sycl and --ur params must be set to some values for the dry-run to properly output metadata for sycl and ur based benchmarks.
          # TODO: remove this once it's addressed in the benchmark scripts.
          (cd ${{ github.workspace }} && ${{ github.workspace }}/sc/devops/scripts/benchmarks/main.py ~/ur_bench_workdir --dry-run --sycl invalid --ur invalid --results-dir ${{ github.workspace }}/results-repo --output-html remote)

        done
